
@article{cricri_video_2016,
	title = {Video {Ladder} {Networks}},
	url = {http://arxiv.org/abs/1612.01756},
	abstract = {We present the Video Ladder Network (VLN) for efﬁciently generating future video frames. VLN is a neural encoder-decoder model augmented at all layers by both recurrent and feedforward lateral connections. At each layer, these connections form a lateral recurrent residual block, where the feedforward connection represents a skip connection and the recurrent connection represents the residual. Thanks to the recurrent connections, the decoder can exploit temporal summaries generated from all layers of the encoder. This way, the top layer is relieved from the pressure of modeling lower-level spatial and temporal details. Furthermore, we extend the basic version of VLN to incorporate ResNet-style residual blocks in the encoder and decoder, which help improving the prediction results. VLN is trained in selfsupervised regime on the Moving MNIST dataset, achieving competitive results while having very simple structure and providing fast inference.},
	language = {en},
	urldate = {2020-10-08},
	journal = {arXiv:1612.01756 [cs, stat]},
	author = {Cricri, Francesco and Ni, Xingyang and Honkala, Mikko and Aksu, Emre and Gabbouj, Moncef},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.01756},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: This version extends the paper accepted at the NIPS 2016 workshop on ML for Spatiotemporal Forecasting, with more details and more experimental results},
	file = {Cricri et al. - 2016 - Video Ladder Networks.pdf:/home/knork/Zotero/storage/DHPMG2S3/Cricri et al. - 2016 - Video Ladder Networks.pdf:application/pdf}
}

@article{tan_survey_2018,
	title = {A {Survey} on {Deep} {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1808.01974},
	abstract = {As a new classiﬁcation platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very diﬃcult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insuﬃcient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We deﬁned deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
	language = {en},
	urldate = {2020-10-08},
	journal = {arXiv:1808.01974 [cs, stat]},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.01974},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The 27th International Conference on Artificial Neural Networks (ICANN 2018)},
	file = {Tan et al. - 2018 - A Survey on Deep Transfer Learning:/home/knork/Zotero/storage/FPGC7N74/Tan et al. - 2018 - A Survey on Deep Transfer Learning:application/pdf}
}

@article{goodfellow_generative_nodate,
	title = {Generative {Adversarial} {Nets}},
	language = {en},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	pages = {9},
	file = {Goodfellow et al. - Generative Adversarial Nets:/home/knork/Zotero/storage/9MGWUM9X/Goodfellow et al. - Generative Adversarial Nets:application/pdf}
}

@article{wang_image_2004,
	title = {Image {Quality} {Assessment}: {From} {Error} {Visibility} to {Structural} {Similarity}},
	volume = {13},
	issn = {1057-7149},
	shorttitle = {Image {Quality} {Assessment}},
	url = {http://ieeexplore.ieee.org/document/1284395/},
	doi = {10.1109/TIP.2003.819861},
	language = {en},
	number = {4},
	urldate = {2020-10-08},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	pages = {600--612},
	file = {Wang et al. - 2004 - Image Quality Assessment From Error Visibility to:/home/knork/Zotero/storage/3PPJP5WE/Wang et al. - 2004 - Image Quality Assessment From Error Visibility to:application/pdf}
}

@misc{aismartz_cnn_2019,
	title = {{CNN} {Architectures} {Timeline} (1998-2019)},
	url = {https://www.aismartz.com/blog/cnn-architectures/},
	abstract = {CNN Architectures (1998-2019) - Let's discuss the most important CNN's used in complex applications like deep learning models \& how they have evolved over time.},
	language = {en},
	urldate = {2020-10-08},
	author = {AISmartz},
	month = oct,
	year = {2019},
	file = {Snapshot:/home/knork/Zotero/storage/K9NHRNXS/cnn-architectures.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:/home/knork/Zotero/storage/FWAB28JI/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}

@misc{tensorflow_how_2020,
	title = {How {Hugging} {Face} achieved a 2x performance boost for {Question} {Answering} with {DistilBERT} in {Node}.js},
	url = {https://blog.tensorflow.org/2020/05/how-hugging-face-achieved-2x-performance-boost-question-answering.html},
	abstract = {The TensorFlow blog contains regular news from the TensorFlow team and the community, with articles on Python, TensorFlow.js, TF Lite, TFX, and more.},
	language = {en},
	urldate = {2020-10-08},
	author = {Tensorflow},
	month = may,
	year = {2020},
	file = {Snapshot:/home/knork/Zotero/storage/NP58BA56/how-hugging-face-achieved-2x-performance-boost-question-answering.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2020-10-08},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/knork/Zotero/storage/7G5JCX5T/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{ma_convgru_nodate,
	title = {{ConvGRU} in {Fine}-grained {Pitching} {Action} {Recognition} for {Action} {Outcome} {Prediction}},
	abstract = {Prediction of the action outcome is a new challenge for a robot collaboratively working with humans. With the impressive progress in video action recognition in recent years, fine-grained action recognition from video data turns into a new concern. Fine-grained action recognition detects subtle differences of actions in more specific granularity and is significant in many fields such as human-robot interaction, intelligent traffic management, sports training, health caring. Considering that the different outcomes are closely connected to the subtle differences in actions, fine-grained action recognition is a practical method for action outcome prediction. In this paper, we explore the performance of convolutional gate recurrent unit (ConvGRU) method on a fine-grained action recognition tasks: predicting outcomes of ball-pitching. Based on sequences of RGB images of human actions, the proposed approach achieved the performance of 79.17\% accuracy, which exceeds the current state-of-the-art result. We also compared different network implementations and showed the influence of different image sampling methods, different fusion methods and pre-training, etc. Finally, we discussed the advantages and limitations of ConvGRU in such action outcome prediction and fine-grained action recognition tasks.},
	language = {en},
	author = {Ma, Tianqi and Zhang, Lin and Diao, Xiumin and Ma, Ou},
	pages = {10},
	file = {Ma et al. - ConvGRU in Fine-grained Pitching Action Recognitio.pdf:/home/knork/Zotero/storage/9PC58QL8/Ma et al. - ConvGRU in Fine-grained Pitching Action Recognitio.pdf:application/pdf}
}

@article{siam_convolutional_2016,
	title = {Convolutional {Gated} {Recurrent} {Networks} for {Video} {Segmentation}},
	url = {http://arxiv.org/abs/1611.05435},
	abstract = {Semantic segmentation has recently witnessed major progress, where fully convolutional neural networks have shown to perform well. However, most of the previous work focused on improving single image segmentation. To our knowledge, no prior work has made use of temporal video information in a recurrent network. In this paper, we introduce a novel approach to implicitly utilize temporal data in videos for online semantic segmentation. The method relies on a fully convolutional network that is embedded into a gated recurrent architecture. This design receives a sequence of consecutive video frames and outputs the segmentation of the last frame. Convolutional gated recurrent networks are used for the recurrent part to preserve spatial connectivities in the image. Our proposed method can be applied in both online and batch segmentation. This architecture is tested for both binary and semantic video segmentation tasks. Experiments are conducted on the recent benchmarks in SegTrack V2, Davis, CityScapes, and Synthia. Using recurrent fully convolutional networks improved the baseline network performance in all of our experiments. Namely, 5\% and 3\% improvement of F-measure in SegTrack2 and Davis respectively, 5.7\% improvement in mean IoU in Synthia and 3.5\% improvement in categorical mean IoU in CityScapes. The performance of the RFCN network depends on its baseline fully convolutional network. Thus RFCN architecture can be seen as a method to improve its baseline segmentation network by exploiting spatiotemporal information in videos.},
	language = {en},
	urldate = {2020-10-08},
	journal = {arXiv:1611.05435 [cs]},
	author = {Siam, Mennatullah and Valipour, Sepehr and Jagersand, Martin and Ray, Nilanjan},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05435},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1606.00487},
	file = {Siam et al. - 2016 - Convolutional Gated Recurrent Networks for Video S.pdf:/home/knork/Zotero/storage/HEBTZ5XK/Siam et al. - 2016 - Convolutional Gated Recurrent Networks for Video S.pdf:application/pdf}
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2020-10-08},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/home/knork/Zotero/storage/JK3YNJSP/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{shi_real-time_2016,
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1609.05158},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single ﬁlter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the ﬁrst convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efﬁcient sub-pixel convolution layer which learns an array of upscaling ﬁlters to upscale the ﬁnal LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic ﬁlter in the SR pipeline with more complex upscaling ﬁlters speciﬁcally trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs signiﬁcantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
	language = {en},
	urldate = {2020-10-08},
	journal = {arXiv:1609.05158 [cs, stat]},
	author = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.05158},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {Comment: CVPR 2016 paper with updated affiliations and supplemental material, fixed typo in equation 4},
	file = {Shi et al. - 2016 - Real-Time Single Image and Video Super-Resolution .pdf:/home/knork/Zotero/storage/E9S73YC9/Shi et al. - 2016 - Real-Time Single Image and Video Super-Resolution .pdf:application/pdf}
}

@incollection{kurkova_location_2018,
	address = {Cham},
	title = {Location {Dependency} in {Video} {Prediction}},
	volume = {11141},
	isbn = {978-3-030-01423-0 978-3-030-01424-7},
	url = {http://link.springer.com/10.1007/978-3-030-01424-7_62},
	abstract = {Deep convolutional neural networks are used to address many computer vision problems, including video prediction. The task of video prediction requires analyzing the video frames, temporally and spatially, and constructing a model of how the environment evolves. Convolutional neural networks are spatially invariant, though, which prevents them from modeling location-dependent patterns. In this work, the authors propose location-biased convolutional layers to overcome this limitation. The eﬀectiveness of location bias is evaluated on two architectures: Video Ladder Network (VLN) and Convolutional Predictive Gating Pyramid (Conv-PGP). The results indicate that encoding location-dependent features is crucial for the task of video prediction. Our proposed methods signiﬁcantly outperform spatially invariant models.},
	language = {en},
	urldate = {2020-10-08},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Azizi, Niloofar and Farazi, Hafez and Behnke, Sven},
	editor = {Kůrková, Věra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2018},
	doi = {10.1007/978-3-030-01424-7_62},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {630--638},
	file = {Azizi et al. - 2018 - Location Dependency in Video Prediction.pdf:/home/knork/Zotero/storage/MRKFJPIH/Azizi et al. - 2018 - Location Dependency in Video Prediction.pdf:application/pdf}
}
